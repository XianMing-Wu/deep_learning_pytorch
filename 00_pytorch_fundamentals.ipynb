{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74c762d-be49-4868-b719-6259ece6990a",
   "metadata": {},
   "source": [
    "首先，先确保pytorch已经安装在我们的环境内，通过如下的命令导入torch，并获取pytorch的版本以及运行环境是cpu还是cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44b9004-5292-4381-9c3b-17978a7c7f1e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:27.131767Z",
     "iopub.status.busy": "2025-10-18T01:41:27.131642Z",
     "iopub.status.idle": "2025-10-18T01:41:28.531034Z",
     "shell.execute_reply": "2025-10-18T01:41:28.530643Z",
     "shell.execute_reply.started": "2025-10-18T01:41:27.131754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719dd670-5b25-4f62-b97c-d7d03242d87f",
   "metadata": {},
   "source": [
    "数据在深度学习里都被我们表示为了tensor张量，比如一张图片就是一个`三维`的tensor张量，比如`[3, 512, 512]`,每个维度的含义分别是`颜色通道数`，`图像高度`，`图像宽度`，可统一表示为`[color channels, height, weight]`。下面我们详细的研究一下pytorch中`标量`，`向量`，`矩阵`，`张量`的一些基本概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d97d12d7-4e02-43aa-af36-76ca274bb373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.532318Z",
     "iopub.status.busy": "2025-10-18T01:41:28.531904Z",
     "iopub.status.idle": "2025-10-18T01:41:28.537339Z",
     "shell.execute_reply": "2025-10-18T01:41:28.537010Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.532299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar标量\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1977344-c3e2-44fa-bf54-7e578a7c9ee1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.537806Z",
     "iopub.status.busy": "2025-10-18T01:41:28.537658Z",
     "iopub.status.idle": "2025-10-18T01:41:28.540370Z",
     "shell.execute_reply": "2025-10-18T01:41:28.540026Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.537790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度: 0\n",
      "元素项: 7\n"
     ]
    }
   ],
   "source": [
    "# 可以通过ndim属性查看一下scalar的维度\n",
    "print(f\"维度: {scalar.ndim}\")\n",
    "# 可以通过item方法查看一下scalasr中的元素项，该方法只有一个元素时才可以使用\n",
    "print(f\"元素项: {scalar.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab159c8-d6ff-4ce2-b7af-152c051b6ece",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.540899Z",
     "iopub.status.busy": "2025-10-18T01:41:28.540760Z",
     "iopub.status.idle": "2025-10-18T01:41:28.544539Z",
     "shell.execute_reply": "2025-10-18T01:41:28.544149Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.540887Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector向量\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cbe2415-aab6-4257-9f7c-028fb3ca8031",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.545088Z",
     "iopub.status.busy": "2025-10-18T01:41:28.544948Z",
     "iopub.status.idle": "2025-10-18T01:41:28.547995Z",
     "shell.execute_reply": "2025-10-18T01:41:28.547498Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.545072Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度: 1\n",
      "形状: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 可以通过ndim属性查看一下vector的维度\n",
    "print(f\"维度: {vector.ndim}\")\n",
    "# 可以通过shape属性查看一下vector的形状\n",
    "print(f\"形状: {vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307ac478-fda5-4241-a6be-db2e018e6a8e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.548519Z",
     "iopub.status.busy": "2025-10-18T01:41:28.548398Z",
     "iopub.status.idle": "2025-10-18T01:41:28.552201Z",
     "shell.execute_reply": "2025-10-18T01:41:28.551723Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.548507Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix矩阵\n",
    "matrix = torch.tensor([[7, 8],\n",
    "                       [9, 10]])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98313cc-3f53-4119-a5e6-208a29870776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.553601Z",
     "iopub.status.busy": "2025-10-18T01:41:28.553428Z",
     "iopub.status.idle": "2025-10-18T01:41:28.556318Z",
     "shell.execute_reply": "2025-10-18T01:41:28.555843Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.553589Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度: 2\n",
      "形状: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# 可以通过ndim属性查看一下matrix的维度\n",
    "print(f\"维度: {matrix.ndim}\")\n",
    "# 可以通过shape属性查看一下matrix的形状\n",
    "print(f\"形状: {matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddb7a44-d575-41f9-b8f3-90818a6cba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.556865Z",
     "iopub.status.busy": "2025-10-18T01:41:28.556736Z",
     "iopub.status.idle": "2025-10-18T01:41:28.561538Z",
     "shell.execute_reply": "2025-10-18T01:41:28.561071Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.556852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor 张量\n",
    "tensor = torch.tensor([[[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]]])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d5eb0c-a4fb-4e37-939a-a8b57e496aa0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.562053Z",
     "iopub.status.busy": "2025-10-18T01:41:28.561928Z",
     "iopub.status.idle": "2025-10-18T01:41:28.564822Z",
     "shell.execute_reply": "2025-10-18T01:41:28.564333Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.562042Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "维度: 3\n",
      "形状: torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 可以通过ndim属性查看一下tensor的维度\n",
    "print(f\"维度: {tensor.ndim}\")\n",
    "# 可以通过shape属性查看一下tensor的形状\n",
    "print(f\"形状: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13e34d-471e-475b-ae5c-d87aad671d74",
   "metadata": {},
   "source": [
    "了解了张量相关的基本概念之后，我们接下来学习`随机张量`，`元素全为0的张量`，`元素全为1的张量`三种常用的张量形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a5eaf6-116b-40f7-ad57-575efb872069",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.565333Z",
     "iopub.status.busy": "2025-10-18T01:41:28.565215Z",
     "iopub.status.idle": "2025-10-18T01:41:28.572608Z",
     "shell.execute_reply": "2025-10-18T01:41:28.572222Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.565322Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64, 64]), 3, torch.float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个(3, 64, 64)的随机张量\n",
    "random_tensor = torch.rand(size=(3, 64, 64))\n",
    "# 查看随机张量的形状，维度，数据类型\n",
    "random_tensor.shape, random_tensor.ndim, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b33473e-a615-4716-b58a-864544b8c008",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.573125Z",
     "iopub.status.busy": "2025-10-18T01:41:28.572988Z",
     "iopub.status.idle": "2025-10-18T01:41:28.576590Z",
     "shell.execute_reply": "2025-10-18T01:41:28.576128Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.573114Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64, 64]), 3, torch.float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和随机张量形状相同的，元素全为0的张量\n",
    "zeros_tensor = torch.zeros(size=random_tensor.shape)\n",
    "# 查看元素全为0的张量的形状，维度，数据类型\n",
    "zeros_tensor.shape, zeros_tensor.ndim, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ace57a2-40b4-46a1-8b7d-10c3f09215f6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.577165Z",
     "iopub.status.busy": "2025-10-18T01:41:28.577026Z",
     "iopub.status.idle": "2025-10-18T01:41:28.580733Z",
     "shell.execute_reply": "2025-10-18T01:41:28.580174Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.577152Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 64, 64]), 3, torch.float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和随机向量形状相同，元素全为1的张量\n",
    "ones_tensor = torch.ones_like(random_tensor)\n",
    "# 查看元素全为0的张量的形状，维度，数据类型\n",
    "ones_tensor.shape, ones_tensor.ndim, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2808c-7ee5-46d7-add5-2ae26c81232c",
   "metadata": {},
   "source": [
    "有时候我们还需要元素大小线性变化的向量用于图像绘制，采样等目的，为此我们还要引入创建这种类型向量的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02e8aad-7cdf-4224-9217-d20b9ad5d979",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.581261Z",
     "iopub.status.busy": "2025-10-18T01:41:28.581141Z",
     "iopub.status.idle": "2025-10-18T01:41:28.587341Z",
     "shell.execute_reply": "2025-10-18T01:41:28.586956Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.581250Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用torch.range(start, end, step)创建一个元素大小线性变化的向量, 其中:\n",
    "# start指的是range的开始值\n",
    "# end指的是range的结束值\n",
    "# step指的是两个元素之间的间隔\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68409c94-409a-4d19-8853-44d5625114c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.587834Z",
     "iopub.status.busy": "2025-10-18T01:41:28.587697Z",
     "iopub.status.idle": "2025-10-18T01:41:28.590805Z",
     "shell.execute_reply": "2025-10-18T01:41:28.590368Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.587819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10]), 1, torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看创建的range的形状，维度，数据类型\n",
    "zero_to_ten.shape, zero_to_ten.ndim, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d8967-99ca-4175-8dc0-b11678974789",
   "metadata": {},
   "source": [
    "## 张量数据类型\n",
    "PyTorch 中有许多不同的张量数据类型。有些是专门用于 CPU 的，有些则更适合 GPU。通常，如果您在任何地方看到torch.cuda，就说明该张量正用于 GPU（因为英伟达 GPU 使用一种名为 CUDA 的计算工具包）。最常见的类型（通常也是默认类型）是torch.float32或torch.float，它被称为 “32 位浮点数”。但也有 16 位浮点数（torch.float16或torch.half）和 64 位浮点数（torch.float64或torch.double）。更复杂的是，还有 8 位、16 位、32 位和 64 位的整数，此外还有更多其他类型！\n",
    "> **注意:** 整数是像7这样的整数，而浮点数带有小数点，如7.0。存在这么多类型的原因与计算精度有关。精度是描述一个数字所使用的细节量。精度值（8、16、32）越高，表达一个数字所使用的细节和数据就越多。这在深度学习和数值计算中很重要，因为您要进行大量运算，计算时使用的细节越多，所需的计算资源就越多。因此，低精度数据类型通常计算速度更快，但会在准确率等评估指标上牺牲一些性能（计算更快但精度更低）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bef8ba-a46c-453d-bbd8-52b78ce1c58c",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.591634Z",
     "iopub.status.busy": "2025-10-18T01:41:28.591322Z",
     "iopub.status.idle": "2025-10-18T01:41:28.594991Z",
     "shell.execute_reply": "2025-10-18T01:41:28.594597Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.591615Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认的tensor数据类型是float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None,\n",
    "                               device=None,\n",
    "                               requires_grad=False)\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885e2482-a8f2-490b-98fe-2dd49ad90197",
   "metadata": {},
   "source": [
    "ok，我们这里先简单的说明一下张量的数据类型，下面我们进入张量的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709974db-b9fe-416b-a1d6-9de62ba5f968",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.595504Z",
     "iopub.status.busy": "2025-10-18T01:41:28.595376Z",
     "iopub.status.idle": "2025-10-18T01:41:28.601940Z",
     "shell.execute_reply": "2025-10-18T01:41:28.601498Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.595492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_a + tensor_b = tensor([[ 3,  6,  9],\n",
      "        [ 5,  8, 13]])\n",
      "tensor_a - tensor_b = tensor([[-1, -2, -3],\n",
      "        [ 3,  2, -1]])\n",
      "tensor_a * tensor_b = tensor([[ 2,  8, 18],\n",
      "        [ 4, 15, 42]])\n"
     ]
    }
   ],
   "source": [
    "# 形状相同的两个张量之间可以直接进行张量加法，减法，以及逐元素对于相乘\n",
    "tensor_a = torch.tensor([[1, 2, 3],\n",
    "                        [4, 5, 6]])\n",
    "tensor_b = torch.tensor([[2, 4, 6],\n",
    "                        [1, 3, 7]])\n",
    "x = tensor_a + tensor_b\n",
    "y = tensor_a - tensor_b\n",
    "z = tensor_a * tensor_b\n",
    "print(f\"tensor_a + tensor_b = {x}\")\n",
    "print(f\"tensor_a - tensor_b = {y}\")\n",
    "print(f\"tensor_a * tensor_b = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dea7c842-3802-4c75-8f22-24637da58184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.602458Z",
     "iopub.status.busy": "2025-10-18T01:41:28.602339Z",
     "iopub.status.idle": "2025-10-18T01:41:28.606083Z",
     "shell.execute_reply": "2025-10-18T01:41:28.605732Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.602447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_c + tensor_b = tensor([[11, 12, 13],\n",
      "        [14, 15, 16]])\n",
      "tensor_c - 10 = tensor([[-9, -8, -7],\n",
      "        [-6, -5, -4]])\n",
      "tensor_c * 10 = tensor([[10, 20, 30],\n",
      "        [40, 50, 60]])\n"
     ]
    }
   ],
   "source": [
    "# tensor还可以与标量通过广播运算实现加法，减法，乘法\n",
    "tensor_c = torch.tensor([[1, 2, 3],\n",
    "                        [4, 5, 6]])\n",
    "x = tensor_c + 10\n",
    "y = tensor_c - 10\n",
    "z = tensor_c * 10\n",
    "print(f\"tensor_c + tensor_b = {x}\")\n",
    "print(f\"tensor_c - 10 = {y}\")\n",
    "print(f\"tensor_c * 10 = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe6abd11-e072-42a4-9aa7-d18bb34918cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.606551Z",
     "iopub.status.busy": "2025-10-18T01:41:28.606434Z",
     "iopub.status.idle": "2025-10-18T01:41:28.610098Z",
     "shell.execute_reply": "2025-10-18T01:41:28.609681Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.606540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_a + tensor_b = tensor([[ 3,  6,  9],\n",
      "        [ 5,  8, 13]])\n",
      "tensor_c - 10 = tensor([[-9, -8, -7],\n",
      "        [-6, -5, -4]])\n",
      "tensor_a * tensor_b = tensor([[ 2,  8, 18],\n",
      "        [ 4, 15, 42]])\n"
     ]
    }
   ],
   "source": [
    "# 上述的加法操作可以直接使用add()方法，减法操作可以直接使用sub()方法，乘法操作可以直接使用mul()方法\n",
    "x = torch.add(tensor_a, tensor_b)\n",
    "y = torch.sub(tensor_c, 10)\n",
    "z = torch.mul(tensor_a, tensor_b)\n",
    "print(f\"tensor_a + tensor_b = {x}\")\n",
    "print(f\"tensor_c - 10 = {y}\")\n",
    "print(f\"tensor_a * tensor_b = {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f081994-3cc0-4e9c-98cd-9d163efcaabd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.610623Z",
     "iopub.status.busy": "2025-10-18T01:41:28.610496Z",
     "iopub.status.idle": "2025-10-18T01:41:28.624606Z",
     "shell.execute_reply": "2025-10-18T01:41:28.624203Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.610611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量a与向量b的点积运算结果如下：32\n",
      "向量a与向量b的点积运算结果如下：32\n",
      "向量a的内积运算结果如下：14\n",
      "向量a的内积运算结果如下：14\n",
      "矩阵a与矩阵b的乘积运算结果如下：tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "矩阵a与矩阵b的乘积运算结果如下：tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "矩阵a与矩阵a的转置的乘积运算结果如下：tensor([[14, 32],\n",
      "        [32, 77]])\n",
      "矩阵a与矩阵a的转置的乘积运算结果如下：tensor([[14, 32],\n",
      "        [32, 77]])\n",
      "矩阵a与矩阵b的乘积运算结果如下：tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "矩阵a与矩阵a的转置的乘积运算结果如下：tensor([[14, 32],\n",
      "        [32, 77]])\n"
     ]
    }
   ],
   "source": [
    "# 同样的我们还可以实现点积运算，内积运算和矩阵乘法\n",
    "vector_a = torch.tensor([1, 2, 3])\n",
    "vector_b = torch.tensor([4, 5, 6])\n",
    "x = vector_a @ vector_b\n",
    "y = torch.matmul(vector_a, vector_b)\n",
    "print(f\"向量a与向量b的点积运算结果如下：{x}\")\n",
    "print(f\"向量a与向量b的点积运算结果如下：{y}\")\n",
    "x = vector_a @ vector_a\n",
    "y = torch.matmul(vector_a, vector_a)\n",
    "print(f\"向量a的内积运算结果如下：{x}\")\n",
    "print(f\"向量a的内积运算结果如下：{y}\")\n",
    "matrix_a = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "matrix_b = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]])\n",
    "x = matrix_a @ matrix_b\n",
    "y = torch.matmul(matrix_a, matrix_b)\n",
    "print(f\"矩阵a与矩阵b的乘积运算结果如下：{x}\")\n",
    "print(f\"矩阵a与矩阵b的乘积运算结果如下：{y}\")\n",
    "x = matrix_a @ matrix_a.T\n",
    "y = torch.matmul(matrix_a, matrix_a.T)\n",
    "print(f\"矩阵a与矩阵a的转置的乘积运算结果如下：{x}\")\n",
    "print(f\"矩阵a与矩阵a的转置的乘积运算结果如下：{y}\")\n",
    "# 矩阵还可以使用mm()方法进行矩阵乘法\n",
    "x = torch.mm(matrix_a, matrix_b)\n",
    "y = torch.mm(matrix_a, matrix_a.T)\n",
    "print(f\"矩阵a与矩阵b的乘积运算结果如下：{x}\")\n",
    "print(f\"矩阵a与矩阵a的转置的乘积运算结果如下：{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481fbfb-43f0-4f21-904f-d5832fa30173",
   "metadata": {},
   "source": [
    "神经网络中充满了矩阵乘法和点积运算。\n",
    "torch.nn.Linear()模块（我们稍后会实际操作它），也被称为前馈层或全连接层，它实现了输入x与权重矩阵A之间的矩阵乘法。\n",
    "$$ y = xA^T+b $$\n",
    "其中：\n",
    "x是该层的输入（深度学习是由像torch.nn.Linear()这样的层以及其他层堆叠而成的）。\n",
    "\n",
    "A是该层创建的权重矩阵，它最初是随机数，随着神经网络学习更好地表示数据中的模式而被调整（注意这里的 “T”，这是因为权重矩阵会被转置）。\n",
    "> 注意：你可能也经常会看到用W或其他字母（如X）来表示权重矩阵。\n",
    "\n",
    "b是偏置项，用于对权重和输入进行轻微偏移。\n",
    "\n",
    "y是输出（对输入的一种处理，目的是发现其中的模式）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46883912-eb0e-4211-9b91-195eb2debbad",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.625231Z",
     "iopub.status.busy": "2025-10-18T01:41:28.625088Z",
     "iopub.status.idle": "2025-10-18T01:41:28.631803Z",
     "shell.execute_reply": "2025-10-18T01:41:28.631456Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.625219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x的形状为：torch.Size([3, 2])\n",
      "输出为：tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "输出的形状为：torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# 为了可以使结果可复现，我们固定随机种子\n",
    "torch.manual_seed(42)\n",
    "# 创建一个线性层，本质上就是创建了一个矩阵A，会涉及到矩阵乘法\n",
    "# 注意这里的in_feature需要和输入x的列匹配\n",
    "linear = torch.nn.Linear(in_features=2,\n",
    "                         out_features=6)\n",
    "# 这里必须将x的类型设置为flaot32,或者元素写成float类型，不然x的默认类型会是int64\n",
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]], dtype=torch.float32)\n",
    "output = linear(x)\n",
    "print(f\"x的形状为：{x.shape}\")\n",
    "print(f\"输出为：{output}\")\n",
    "print(f\"输出的形状为：{output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdb07c-f93b-4a8f-8b41-ed9ef3974330",
   "metadata": {},
   "source": [
    "ok，到现在为止基本的张量运算我们已经探讨完毕，接下来我们研究一下对于给定张量，如何求最大，最小，求平均，以及求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b1b54b-815b-4702-bd37-0e2ff6578e48",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.632298Z",
     "iopub.status.busy": "2025-10-18T01:41:28.632177Z",
     "iopub.status.idle": "2025-10-18T01:41:28.635160Z",
     "shell.execute_reply": "2025-10-18T01:41:28.634824Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.632286Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们先来探究一下对于向量而言的最大，最小，平均以及求和\n",
    "x = torch.arange(0, 100, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a679d5a-f91b-46c1-9da7-031b3d02da53",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.637478Z",
     "iopub.status.busy": "2025-10-18T01:41:28.637159Z",
     "iopub.status.idle": "2025-10-18T01:41:28.640448Z",
     "shell.execute_reply": "2025-10-18T01:41:28.640042Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.637465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0\n",
      "Maximum: 90\n",
      "最小元素对应索引：0\n",
      "最大元素对应索引：9\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "print(f\"最小元素对应索引：{x.argmin()}\")\n",
    "print(f\"最大元素对应索引：{x.argmax()}\")\n",
    "# 注意求平均必须是float类型，此时的x是int64，必须进行类型转换\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\")\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7bd056b-1c46-4384-a6a6-4d35114c0d08",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.641079Z",
     "iopub.status.busy": "2025-10-18T01:41:28.640844Z",
     "iopub.status.idle": "2025-10-18T01:41:28.644082Z",
     "shell.execute_reply": "2025-10-18T01:41:28.643693Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.641066Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 3, 8],\n",
       "        [4, 2, 6, 1],\n",
       "        [3, 1, 7, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来我们再来看一看矩阵的最大，最小，平均以及求和\n",
    "x = torch.tensor([[1, 5, 3, 8],\n",
    "                  [4, 2, 6, 1],\n",
    "                  [3, 1, 7, 0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46bfc938-fbfc-4aec-9e00-1984d3fd975a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.644600Z",
     "iopub.status.busy": "2025-10-18T01:41:28.644474Z",
     "iopub.status.idle": "2025-10-18T01:41:28.654087Z",
     "shell.execute_reply": "2025-10-18T01:41:28.653672Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.644589Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对行求最小：tensor([1, 1, 3, 0])\n",
      "对列求最小：tensor([1, 1, 0])\n",
      "对行求最小索引列表：tensor([0, 2, 0, 2])\n",
      "对列求最小索引列表：tensor([0, 3, 3])\n",
      "对行求最大：tensor([4, 5, 7, 8])\n",
      "对列求最大：tensor([8, 6, 7])\n",
      "对行求最大索引列表：tensor([1, 0, 2, 0])\n",
      "对列求最大索引列表：tensor([3, 2, 2])\n",
      "对行求平均：tensor([2.6667, 2.6667, 5.3333, 3.0000])\n",
      "对列求平均：tensor([4.2500, 3.2500, 2.7500])\n",
      "对行求和：tensor([ 8,  8, 16,  9])\n",
      "对列求和：tensor([17, 13, 11])\n"
     ]
    }
   ],
   "source": [
    "# 此时的最大，最小，平均以及求和就要考虑是对行，还是对列，分别对应dim=0，dim=1\n",
    "print(f\"对行求最小：{x.min(dim=0).values}\")\n",
    "print(f\"对列求最小：{x.min(dim=1).values}\")\n",
    "print(f\"对行求最小索引列表：{x.min(dim=0).indices}\")\n",
    "print(f\"对列求最小索引列表：{x.min(dim=1).indices}\")\n",
    "print(f\"对行求最大：{x.max(dim=0).values}\")\n",
    "print(f\"对列求最大：{x.max(dim=1).values}\")\n",
    "print(f\"对行求最大索引列表：{x.max(dim=0).indices}\")\n",
    "print(f\"对列求最大索引列表：{x.max(dim=1).indices}\")\n",
    "print(f\"对行求平均：{x.type(torch.float32).mean(dim=0)}\")\n",
    "print(f\"对列求平均：{x.type(torch.float32).mean(dim=1)}\")\n",
    "print(f\"对行求和：{x.sum(dim=0)}\")\n",
    "print(f\"对列求和：{x.sum(dim=1)}\")\n",
    "# 当然也可以求所有元素的最小，最大，所有元素的平均，求和，去掉dim限制即可，这里不再继续说明\n",
    "# 对于张量而言的话，搞清楚维度顺序即可，下面也不继续阐述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08add39-ff0b-48f8-9d38-d4761d7226e4",
   "metadata": {},
   "source": [
    "接下来我们详细的讨论一下tensor的数据类型转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d90c3287-914c-4f6a-a150-07ba64ad8893",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.654578Z",
     "iopub.status.busy": "2025-10-18T01:41:28.654439Z",
     "iopub.status.idle": "2025-10-18T01:41:28.658280Z",
     "shell.execute_reply": "2025-10-18T01:41:28.657831Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.654564Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面两种方式定义的张量数据类型是不一样的\n",
    "tensor_a = torch.arange(10., 100., 10.)\n",
    "tensor_b = torch.arange(10, 100, 10)\n",
    "tensor_a.dtype, tensor_b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb9d51a6-62cd-4d43-bcad-f8069a405232",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.658967Z",
     "iopub.status.busy": "2025-10-18T01:41:28.658762Z",
     "iopub.status.idle": "2025-10-18T01:41:28.663761Z",
     "shell.execute_reply": "2025-10-18T01:41:28.663388Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.658955Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float16, torch.int8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过type()方法可以改变张量的数据类型\n",
    "tensor_a_float16 = tensor_a.type(torch.float16)\n",
    "tensor_b_int8 = tensor_b.type(torch.int8)\n",
    "tensor_a_float16.dtype, tensor_b_int8.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf0058-3321-4ab4-8c8a-04e66a81e52b",
   "metadata": {},
   "source": [
    "有时候我们需要在张量内部值不变的情况下对张量进行`重塑`，`堆叠`，`维度调整`，那么就会涉及到`Reshape`, `view`, `permute`, `stack`, `squeeze`, `unsqueeze`这些操作，下面我们依次看一下这些操作带来的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7ee7759-88f9-4bb7-8a14-8c1ed89b5eb8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.664341Z",
     "iopub.status.busy": "2025-10-18T01:41:28.664178Z",
     "iopub.status.idle": "2025-10-18T01:41:28.668259Z",
     "shell.execute_reply": "2025-10-18T01:41:28.667892Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.664329Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原来的张量是：tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9],\n",
      "        [8, 7, 6]])，原来的形状是：torch.Size([4, 3])\n",
      "使用reshape重塑后的张量是：tensor([[1, 2, 3, 4, 5, 6],\n",
      "        [7, 8, 9, 8, 7, 6]]), 重塑后的形状是：torch.Size([2, 6])\n",
      "使用view重塑后的张量是：tensor([[1, 2, 3, 4, 5, 6],\n",
      "        [7, 8, 9, 8, 7, 6]]), 重塑后的形状是：torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "# Reshape(),view()两种方法都可以对张量形状进行重塑\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9],\n",
    "                       [8, 7, 6]])\n",
    "print(f\"原来的张量是：{tensor}，原来的形状是：{tensor.shape}\")\n",
    "tensor_a = tensor.reshape(2, 6)\n",
    "print(f\"使用reshape重塑后的张量是：{tensor_a}, 重塑后的形状是：{tensor_a.shape}\")\n",
    "tensor_b = tensor.view(2, 6)\n",
    "print(f\"使用view重塑后的张量是：{tensor_b}, 重塑后的形状是：{tensor_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2bcf931-6f51-4b3e-a346-0dc025f078b5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.668856Z",
     "iopub.status.busy": "2025-10-18T01:41:28.668676Z",
     "iopub.status.idle": "2025-10-18T01:41:28.673214Z",
     "shell.execute_reply": "2025-10-18T01:41:28.672841Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.668843Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "沿着dim0堆叠：\n",
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]],\n",
      "\n",
      "        [[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "沿着dim1堆叠：\n",
      "tensor([[[1, 2, 3],\n",
      "         [1, 2, 3],\n",
      "         [1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6],\n",
      "         [4, 5, 6],\n",
      "         [4, 5, 6]],\n",
      "\n",
      "        [[7, 8, 9],\n",
      "         [7, 8, 9],\n",
      "         [7, 8, 9]]])\n",
      "沿着dim2堆叠：\n",
      "tensor([[[1, 1, 1],\n",
      "         [2, 2, 2],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[4, 4, 4],\n",
      "         [5, 5, 5],\n",
      "         [6, 6, 6]],\n",
      "\n",
      "        [[7, 7, 7],\n",
      "         [8, 8, 8],\n",
      "         [9, 9, 9]]])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以使用stack对张量进行堆叠\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "tensor_stack_0 = torch.stack([tensor, tensor, tensor], dim=0)\n",
    "tensor_stack_1 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "tensor_stack_2 = torch.stack([tensor, tensor, tensor], dim=2)\n",
    "print(f\"沿着dim0堆叠：\\n{tensor_stack_0}\")\n",
    "print(f\"沿着dim1堆叠：\\n{tensor_stack_1}\")\n",
    "print(f\"沿着dim2堆叠：\\n{tensor_stack_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec1062af-000e-45a5-a196-b9520c0bec1f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.673876Z",
     "iopub.status.busy": "2025-10-18T01:41:28.673630Z",
     "iopub.status.idle": "2025-10-18T01:41:28.677218Z",
     "shell.execute_reply": "2025-10-18T01:41:28.676840Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.673863Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原来的张量如下：tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])，形状如下：torch.Size([1, 3, 3])\n",
      "压缩后的张量如下：tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])，形状如下：torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以使用squeeze()方法压缩张量，只保留大于1的维度\n",
    "tensor = torch.tensor([[[1, 2, 3],\n",
    "                        [4, 5, 6],\n",
    "                        [7, 8, 9]]])\n",
    "tensor_squeeze = tensor.squeeze()\n",
    "print(f\"原来的张量如下：{tensor}，形状如下：{tensor.shape}\")\n",
    "print(f\"压缩后的张量如下：{tensor_squeeze}，形状如下：{tensor_squeeze.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c411a1dd-8fd7-41a3-8e24-3c8caa317614",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.677966Z",
     "iopub.status.busy": "2025-10-18T01:41:28.677636Z",
     "iopub.status.idle": "2025-10-18T01:41:28.681322Z",
     "shell.execute_reply": "2025-10-18T01:41:28.680945Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.677953Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原来的张量如下：tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])，形状如下：torch.Size([3, 3])\n",
      "在dim=0的位置添加值为1的维度后如下：tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])，形状如下：torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# 与squeeze()方法相反，我们还可以使用unsqueeze()方法在特定索引处添加一个值为1的维度\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "tensor_unsqueeze = tensor.unsqueeze(dim=0)\n",
    "print(f\"原来的张量如下：{tensor}，形状如下：{tensor.shape}\")\n",
    "print(f\"在dim=0的位置添加值为1的维度后如下：{tensor_unsqueeze}，形状如下：{tensor_unsqueeze.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72ddf7d3-996f-4e3f-8310-b8be38ec37f9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.682051Z",
     "iopub.status.busy": "2025-10-18T01:41:28.681745Z",
     "iopub.status.idle": "2025-10-18T01:41:28.685102Z",
     "shell.execute_reply": "2025-10-18T01:41:28.684735Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.682038Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以使用permute调整维度顺序\n",
    "tensor = torch.arange(1, 10, 1).reshape(1, 1, 3, 3)\n",
    "tensor_permute = tensor.permute(0, 2, 3, 1)\n",
    "tensor_permute.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c5d57e-e7c0-406b-8af7-d939f33c4739",
   "metadata": {},
   "source": [
    "我们接下来再研究一下索引，切片这些概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "625f447e-630c-450e-ad51-39a006b6f6e5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.685818Z",
     "iopub.status.busy": "2025-10-18T01:41:28.685533Z",
     "iopub.status.idle": "2025-10-18T01:41:28.690343Z",
     "shell.execute_reply": "2025-10-18T01:41:28.689969Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.685801Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量如下：\n",
      "tensor([[[[ 0,  1],\n",
      "          [ 2,  3]],\n",
      "\n",
      "         [[ 4,  5],\n",
      "          [ 6,  7]],\n",
      "\n",
      "         [[ 8,  9],\n",
      "          [10, 11]]]])\n",
      "第一层：\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[ 8,  9],\n",
      "         [10, 11]]])\n",
      "第二层：\n",
      "tensor([[0, 1],\n",
      "        [2, 3]])\n",
      "第三层：\n",
      "tensor([0, 1])\n",
      "第四层：\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0, 12, 1).reshape(1, 3, 2, 2)\n",
    "print(f\"张量如下：\\n{tensor}\")\n",
    "print(f\"第一层：\\n{tensor[0]}\")\n",
    "print(f\"第二层：\\n{tensor[0][0]}\")\n",
    "print(f\"第三层：\\n{tensor[0][0][0]}\")\n",
    "print(f\"第四层：\\n{tensor[0][0][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13384906-3000-489c-a195-bb3e2c58f926",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.691213Z",
     "iopub.status.busy": "2025-10-18T01:41:28.690848Z",
     "iopub.status.idle": "2025-10-18T01:41:28.695561Z",
     "shell.execute_reply": "2025-10-18T01:41:28.695179Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.691190Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保留向量前两个元素以外的所有元素：tensor([3, 4, 5, 6, 7, 8, 9])\n",
      "只保留向量前两个元素：tensor([1, 2])\n",
      "保留向量全部元素：tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "获取向量中中间隔1个元素的所有元素：tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "# 对于切片我们从向量开始，一步步增加难度\n",
    "vector = torch.arange(1, 10, 1)\n",
    "print(f\"保留向量前两个元素以外的所有元素：{vector[2:]}\")\n",
    "print(f\"只保留向量前两个元素：{vector[:2]}\")\n",
    "print(f\"保留向量全部元素：{vector[:]}\")\n",
    "print(f\"获取向量中中间隔1个元素的所有元素：{vector[::2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01c852ee-b61a-40b7-9492-81102ae87335",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.696346Z",
     "iopub.status.busy": "2025-10-18T01:41:28.696038Z",
     "iopub.status.idle": "2025-10-18T01:41:28.699604Z",
     "shell.execute_reply": "2025-10-18T01:41:28.699114Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.696323Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保留矩阵第一行以外的所有行，每行内，列按照中间各一个元素取：\n",
      "tensor([[ 4,  6],\n",
      "        [ 8, 10]])\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.arange(0, 12, 1).reshape(3, 4)\n",
    "print(f\"保留矩阵第一行以外的所有行，每行内，列按照中间各一个元素取：\\n{matrix[1:, ::2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654235c-7ca4-4bca-a66d-fceba2021213",
   "metadata": {},
   "source": [
    "紧接着我们来看一下tensor与numpy的相互转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cb68b3c-f9af-4702-a2ae-2214cb46b149",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.700550Z",
     "iopub.status.busy": "2025-10-18T01:41:28.700202Z",
     "iopub.status.idle": "2025-10-18T01:41:28.704459Z",
     "shell.execute_reply": "2025-10-18T01:41:28.703969Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.700527Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'), torch.int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor与numpy的相互转化主要涉及两个方法，一个用于tensor -> numpy，一个用于numpy -> tensor\n",
    "# numpy -> tensor\n",
    "numpy = np.arange(0, 12, 1).reshape(3, 4)\n",
    "tensor = torch.from_numpy(numpy)\n",
    "numpy.dtype, tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b4c4d18-1398-41e0-81af-8710f5dc3cb6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.705209Z",
     "iopub.status.busy": "2025-10-18T01:41:28.704997Z",
     "iopub.status.idle": "2025-10-18T01:41:28.709028Z",
     "shell.execute_reply": "2025-10-18T01:41:28.708517Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.705188Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'), torch.int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor -> numpy\n",
    "tensor = torch.arange(0, 12, 1).reshape(3, 4)\n",
    "numpy = torch.Tensor.numpy(tensor)\n",
    "numpy.dtype, tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f7a8e-320d-4749-b7d2-d0bb308d01be",
   "metadata": {},
   "source": [
    "> **注意：** 默认情况下，NumPy数组创建时的数据类型为`float64`，如果将其转换为PyTorch张量，它会保持相同的数据类型（如上所述）。\n",
    ">\n",
    "> 然而，许多PyTorch计算默认使用`float32`。\n",
    ">\n",
    "> 因此，如果你想将NumPy数组（float64）转换为PyTorch张量（float64）再转换为PyTorch张量（float32），可以使用`tensor = torch.from_numpy(array).type(torch.float32)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fce8ccf9-a909-4d16-8e68-b6f6a623b4ea",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.709929Z",
     "iopub.status.busy": "2025-10-18T01:41:28.709703Z",
     "iopub.status.idle": "2025-10-18T01:41:28.713531Z",
     "shell.execute_reply": "2025-10-18T01:41:28.713095Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.709908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, dtype('float32'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 也可以通过这种方法将tensor -> numpy，需要注意的是，转换前后的类型会保持一直，除非使用type方法再次进行转换\n",
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor.dtype, numpy_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452870f3-4c5c-48de-aaf2-90599b1e6e8c",
   "metadata": {},
   "source": [
    "为了能够使得我们的实验是完全可以复现的，只要是涉及到随机化，我们都需要固定我们的随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7fc3ec5-a1d8-4e39-9362-b4c55fb4f9b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.714412Z",
     "iopub.status.busy": "2025-10-18T01:41:28.714213Z",
     "iopub.status.idle": "2025-10-18T01:41:28.720079Z",
     "shell.execute_reply": "2025-10-18T01:41:28.719628Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.714392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.2666, 0.6274, 0.2696, 0.4414],\n",
      "        [0.2969, 0.8317, 0.1053, 0.2695],\n",
      "        [0.3588, 0.1994, 0.5472, 0.0062]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.9516, 0.0753, 0.8860, 0.5832],\n",
      "        [0.3376, 0.8090, 0.5779, 0.9040],\n",
      "        [0.5547, 0.3423, 0.6343, 0.3644]])\n",
      "\n",
      "两个张量相等吗?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(\"两个张量相等吗?\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f14660fc-8955-4842-b10d-338e13252d46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.720881Z",
     "iopub.status.busy": "2025-10-18T01:41:28.720652Z",
     "iopub.status.idle": "2025-10-18T01:41:28.727278Z",
     "shell.execute_reply": "2025-10-18T01:41:28.726838Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.720857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.8694, 0.5677, 0.7411, 0.4294],\n",
      "        [0.8854, 0.5739, 0.2666, 0.6274],\n",
      "        [0.2696, 0.4414, 0.2969, 0.8317]])\n",
      "\n",
      "两个张量相等吗?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(\"两个张量相等吗?\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aed9242-2dd5-4c70-b72f-93aa4f175299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.728089Z",
     "iopub.status.busy": "2025-10-18T01:41:28.727878Z",
     "iopub.status.idle": "2025-10-18T01:41:28.734806Z",
     "shell.execute_reply": "2025-10-18T01:41:28.734358Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.728069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "两个张量相等吗?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(\"两个张量相等吗?\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329176fa-6914-425b-b55e-6aed508e3c45",
   "metadata": {},
   "source": [
    "最后我们看一下与gpu相关的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bb96ef1-6191-49ad-91a7-daa046629707",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.735613Z",
     "iopub.status.busy": "2025-10-18T01:41:28.735408Z",
     "iopub.status.idle": "2025-10-18T01:41:28.974077Z",
     "shell.execute_reply": "2025-10-18T01:41:28.973549Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.735592Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 18 09:41:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A10                     Off |   00000000:00:07.0 Off |                  Off |\n",
      "|  0%   27C    P8             19W /  150W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# 查看一下我们的显卡相关信息\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5f38ec7-7d4b-4bc1-8836-aa68a60b5346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:28.974923Z",
     "iopub.status.busy": "2025-10-18T01:41:28.974676Z",
     "iopub.status.idle": "2025-10-18T01:41:28.996033Z",
     "shell.execute_reply": "2025-10-18T01:41:28.995669Z",
     "shell.execute_reply.started": "2025-10-18T01:41:28.974900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d369079-1e74-4922-b039-1f48b4f0cbea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:41:43.871795Z",
     "iopub.status.busy": "2025-10-18T01:41:43.871510Z",
     "iopub.status.idle": "2025-10-18T01:41:43.874914Z",
     "shell.execute_reply": "2025-10-18T01:41:43.874523Z",
     "shell.execute_reply.started": "2025-10-18T01:41:43.871779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置我们的设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fed1a0e9-39bf-4fa9-98c9-ff34f2f7d018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:42:06.468050Z",
     "iopub.status.busy": "2025-10-18T01:42:06.467788Z",
     "iopub.status.idle": "2025-10-18T01:42:06.483091Z",
     "shell.execute_reply": "2025-10-18T01:42:06.482700Z",
     "shell.execute_reply.started": "2025-10-18T01:42:06.468034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看我们的GPU数量\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68c709-3892-42f7-99ce-c732f37b97b2",
   "metadata": {},
   "source": [
    "### 将张量（和模型）放到GPU上\n",
    "\n",
    "你可以通过对张量（以及后面会讲到的模型）调用[`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html)方法，将它们放到特定的设备上。其中，`device`是你希望张量（或模型）转移到的目标设备。\n",
    "\n",
    "为什么要这么做？\n",
    "\n",
    "GPU提供的数值计算速度远比CPU快，而且如果GPU不可用，由于我们的**设备无关代码**（见上文），程序会在CPU上运行。\n",
    "\n",
    "> **注意：** 使用`to(device)`将张量放到GPU上（例如`some_tensor.to(device)`）会返回该张量的一个副本，也就是说，同一个张量会同时存在于CPU和GPU上。要覆盖原张量，需要重新赋值：\n",
    ">\n",
    "> `some_tensor = some_tensor.to(device)`\n",
    "\n",
    "让我们尝试创建一个张量并将其放到GPU上（如果GPU可用的话）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4ca8607-ff35-41fc-8fc4-80106309a7c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-18T01:44:43.538437Z",
     "iopub.status.busy": "2025-10-18T01:44:43.538167Z",
     "iopub.status.idle": "2025-10-18T01:44:43.620110Z",
     "shell.execute_reply": "2025-10-18T01:44:43.619729Z",
     "shell.execute_reply.started": "2025-10-18T01:44:43.538419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个张量（默认是在cpu上的）\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 查看张量以及张量所在的设备\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# 将张量移动到GPU上\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc53fb3f-ef3b-4977-8ac9-120175efa008",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-10-18T01:48:01.875795Z",
     "iopub.status.busy": "2025-10-18T01:48:01.875499Z",
     "iopub.status.idle": "2025-10-18T01:48:01.879204Z",
     "shell.execute_reply": "2025-10-18T01:48:01.878736Z",
     "shell.execute_reply.started": "2025-10-18T01:48:01.875779Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当张量被移动到GPU上的时候，我们无法直接将其转化为numpy，需要先移动到cpu上才可以转化为numpy\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
